# Script for Week 8a

Lesson notes
- Parametric and non-parametric models

Parametric - the mathematical functions relating predictors and output.
Non-parametric - divide the predictors space into small regions and then use simple models.

Terminologies in decision tree
- Root node
- Branch
- Leaf, bucket, terminal
- Decisions, made with one variable and one cut-point

The regions are non-overlapping (divide into small regions),  
Classification - majority vote.
Regression - take average.

Benefits of a decision tree
- (Small) decision trees are relatively easy to interpret.
- Flexible - can be used to predict continuous, binary and multiclass. 
- Computationally efficient for the preformance.


Start with all observations in one region.
Choose predictor and cut-point such that

$$
\underset{\{p\}}{\min} \underset{S}{\min}
\left[
\sum_{i: x_{is} \leq S} (y_i - \hat{y}_{R_1})
+
\sum_{i: x_{is} > S} (y_i - \hat{y}_{R_1})
\right]
$$

Solve a sequence of locally optimal problems with exhaustive search. The cut point is one of the the data points, along one of the dimensions.

Repeat until one exit condition is met.
- For example, the minimum size of the bucket of leaves

$$
\underset{\{R\}}{\min} \sum_{m=1}^M \sum_{i \in R_m} (y_i - \hat{y}_{R_M})^2
$$

## Data Preparation
```{r}
# Load the data on the supreme court
supreme <- read.csv("wk8-law.csv") # We have 623 observations and 20 variables
head(supreme) # First part of the dataframe
# str(supreme) # Internal structure of the dataframe 
summary(supreme) # Summary of the data
```

0 if a liberal decision was made, 1 if a conservative decision was made.

A few comments on the data:
- docket: case number
- term: year the case was discussed
- party_1 and party_2: parties involved in the case
- rehndir stevdir ocondir scaldir kendir soutdir thomdir gindir brydir: direction of the judgement of the 9 judges
- (i.e., Rehquist, Stevens, O'Connor, Scalia, Kennedy, Souter, Thomor, Ginsburg, Breyen). 0 means liberal, 1 conservative. 
- 9 indicates that the vote is not available for the case.
- petit: petitioner type
- respon: respondant type
- circuit: circuit of origin
- uncost: binary number indicating if the petitioner argued constitutionality of a law or practice
- lctdir: lower court direction of results (liberal or conservative)
- issue: area of issue
- result: 0 means liberal, 1 conservative.


```{r}
# Let's now focus on a specific judge, say Stevens (we remove data with no entry from Stevens--Stevens was present)
stevens <- subset(supreme[,c("docket","term","stevdir","petit",
                            "respon","circuit","unconst",
                            "lctdir","issue","result")],
                  supreme$stevdir!=9)
```

# This creates a new variable ($rev) in the dataframe that takes a value of 
- 1 if Stevens decision reverses the lower court decision
- 0 if the judge affirms the lower court decision. 
Note that a similar analysis can be done for the other judges or for the overall case result.
```{r}
# Processing the output result to affirm or reverse for judge Stevens
stevens$rev <- as.integer((stevens$lctdir=="conser" & stevens$stevdir==0) | (stevens$lctdir=="liberal" & stevens$stevdir==1))
table(stevens$rev)
```


```{r}
######## Modelling exercise: logistic regression

# Let's prepare the data for our modelling exercise
# Load the caTools package (basic utility functions)
if(!require(caTools)){
  install.packages("caTools")
  library(caTools)
}
# Set the seed
set.seed(1)
# Create train and test sets (with balanced response from the judge Stevens)
spl <- sample.split(stevens$rev,SplitRatio=0.7) # We use 70% of the data for training
train <- subset(stevens,spl==TRUE); # table(train$rev)
test <- subset(stevens,spl==FALSE); # table(test$rev)
# There is only one realization of the IR value (Interstate Relations), which does not
# appear in the training dataset. Let's thus modify the test dataset as follows:
test <- subset(test,test$issue!="IR")
test
```


We predict judge Stevens' decision based on logistic regression
```{r}
m1 <- glm(rev~unconst+rev+petit+circuit+lctdir+issue,
          data=train,
          family="binomial")
summary(m1)
# let's now try to make a prediction on the test data
p1 <- predict(m1,
              newdata=test,
              type="response")

t <- table(p1 >= 0.5, test$rev)
t
(t[1,1] + t[2,2]) / sum(t)

library(ROCR)
ROCRpred <- prediction(p1,test$rev)
ROCRperf <- performance(ROCRpred,x.measure="fpr",measure="tpr")
plot(ROCRperf,
     colorize=T,
     print.cutoffs.at=c(0,0.2,0.4,0.6,0.8,1.0),
     text.adj=c(-0.2,1.7))
as.numeric(performance(ROCRpred,measure="auc")@y.values)
```

```{r}
# How does the model perform?
table(p1>=0.5,test$rev)
# Confusion matrix
#        0  1
# FALSE ...
# TRUE  ...
# The accuracy is about ... 

# Area Under the Curve
if(!require(ROCR)){
  install.packages("ROCR")
  library(ROCR)
}
pred <- ...
perf <- ...
plot(perf) 
performance(pred,measure="auc")
```

### Classification And Regression Trees (CARTs)
```{r}
# rpart is for CART
if(!require(rpart)){
  install.packages("rpart")
  library(rpart)
}
# rpart.plot extend some functionalities of rpart
if(!require(rpart.plot)){
  install.packages("rpart.plot")
  library(rpart.plot)
}

# # This will not work, because rpart will build a regression tree by default
# cart1 <- rpart(rev~petit+respon+circuit+lctdir+issue+unconst,data=train)
# summary(cart1)
```

